{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e53dc4f6",
   "metadata": {},
   "source": [
    "<div >\n",
    "<img src = \"../banner.jpg\" />\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fa1c83b",
   "metadata": {},
   "source": [
    "<a target=\"_blank\" href=\"https://colab.research.google.com/github/ignaciomsarmiento/BDML_202501/blob/main/Modulo01/CuadernoModulo01_Coef_Computation.ipynb\">\n",
    "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
    "</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32d87f3f",
   "metadata": {},
   "source": [
    "# Computing the OLS Coefficients\n",
    "\n",
    "- Using matrix algebra, the loss function:\n",
    "\n",
    "$$\n",
    "\\tilde \\epsilon' \\tilde \\epsilon  = (y-X \\tilde \\beta)'(y-X \\tilde \\beta)\n",
    "$$\n",
    "\n",
    "  - $SSR(\\tilde \\beta)$ is the aggregation of squared errors if we choose $\\tilde \\beta$ as an estimator.\n",
    "\n",
    "- The **least squares estimator $\\hat \\beta$** will be:\n",
    "\n",
    "$$\n",
    "\\hat \\beta = \\underset{\\tilde \\beta}{\\text{argmin}}\\, SSR(\\tilde \\beta)\n",
    "$$\n",
    "\n",
    "## Traditional Computation using Normal Equations\n",
    "\n",
    "\n",
    "### First-Order Conditions (FOC)\n",
    "\n",
    "- The FOC are:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\tilde{e}' \\tilde{e}}{\\partial \\tilde{\\beta}} = 0\n",
    "$$\n",
    "\n",
    "$$\n",
    "-2X'y + 2X'X \\tilde{\\beta} = 0\n",
    "$$\n",
    "\n",
    "- Second-Order Conditions (SOC) (**Homework**)\n",
    "\n",
    "- Let $\\hat{\\beta}$ be the solution. Then $\\hat{\\beta}$ satisfies the following normal equation:\n",
    "\n",
    "$$\n",
    "X'X\\hat{\\beta} = X'y\n",
    "$$\n",
    "\n",
    "- If the inverse of $X'X$ exists, then:\n",
    "\n",
    "$$\n",
    "\\hat{\\beta} = (X'X)^{-1}X'y\n",
    "$$\n",
    "\n",
    "### Pros:\n",
    "- Closed solution (a bonus!!)\n",
    "\n",
    "### Cons:\n",
    "- Involves inverting a $K \\times K$ matrix $X'X$.\n",
    "\n",
    "Let's illustrate this in R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4b00b42",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# install.packages(\"pacman\") #Correr esta linea en Google Colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d86d5977",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Cargamos los paquetes\n",
    "\n",
    "require(\"pacman\")\n",
    "\n",
    "p_load(\"tidyverse\",\"stargazer\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6acb141f",
   "metadata": {},
   "source": [
    "Creamos unos datos de juguete para ilustrar:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f4546c8",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "dta<-tibble(lnwage=c(5,10,12.50),educ=c(8,12,16))\n",
    "dta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e06f9f05-531a-4b0c-9fde-6c85a5eb0f06",
   "metadata": {},
   "outputs": [],
   "source": [
    "ggplot(dta,aes(x=educ,y=lnwage)) +\n",
    "  geom_point(alpha=1,size=4) +\n",
    "  theme_bw()  +\n",
    "  xlab(\"Education\") +\n",
    "  ylab(\"Ln(Wage)\") +\n",
    "  xlim(0,20) +\n",
    "  ylim(-5,20) +\n",
    "  theme(legend.position = \"none\",\n",
    "        panel.grid.major = element_blank(),\n",
    "        panel.grid.minor = element_blank(),\n",
    "        text = element_text(size=20)\n",
    "  )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc390c9d-efda-42af-b5f4-416cbd2d3f3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dependant variable as a one-column matrix, i.e., a vector\n",
    "y<-matrix(dta$lnwage,ncol=1)\n",
    "\n",
    "# the design matrix\n",
    "X<-model.matrix(~educ,data=dta)\n",
    "\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c9df846-5226-49ba-9758-7fc2b7af70ce",
   "metadata": {},
   "source": [
    "$$\n",
    "\\hat{\\beta} = (X'X)^{-1}X'y\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29246452-7d6f-4670-8039-6d44ed143670",
   "metadata": {},
   "outputs": [],
   "source": [
    "beta_coef<-solve(t(X)%*%X)%*%t(X)%*%y\n",
    "\n",
    "beta_coef"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2711246-5a31-4f76-bf4c-d7dbeb180bf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_with_lm<- lm(lnwage~educ,data=dta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd80eb02-b5e1-4e3b-bd89-3fa50cad94e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_with_lm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4a444e6-e643-4ce4-89a7-1a8de906306b",
   "metadata": {},
   "outputs": [],
   "source": [
    "str(reg_with_lm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29fcf906-5854-419f-b7bf-97083d7d9533",
   "metadata": {},
   "source": [
    "## Traditional Computation using QR Decomposition\n",
    "\n",
    "- To avoid inverting $X'X$, we can use matrix decomposition: **QR decomposition**.\n",
    "- Most software uses this technique.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e7e28f1-eb1f-41db-9a93-7567fc2d6be1",
   "metadata": {},
   "outputs": [],
   "source": [
    "lm.fit(X,y)$coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0747ed8a-1314-4bde-b18a-c454cc9879e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#?lm.fit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26e77873-ef12-4c34-8138-568831a3647c",
   "metadata": {},
   "source": [
    "**Theorem:**\n",
    "If $A \\in \\mathbb{R}^{n \\times k}$, there exists an orthogonal $Q \\in \\mathbb{R}^{n \\times k}$ and an upper triangular $R \\in \\mathbb{R}^{k \\times k}$ such that $A = QR$.\n",
    "\n",
    "#### Properties:\n",
    "- Orthogonal Matrices:\n",
    "  - Definition: $Q'Q = QQ' = I$ and $Q' = Q^{-1}$\n",
    "  - Property: The product of orthogonal matrices is orthogonal. For example, if $A'A = I$ and $B'B = I$, then $(AB)'(AB) = B'(A'A)B = B'B = I$.\n",
    "- **Thin QR:** If $A \\in \\mathbb{R}^{n \\times k}$ has full column rank, then $A = Q_1R_1$, where $Q_1 \\in \\mathbb{R}^{n \\times k}$, and $R$ is upper triangular with positive diagonal entries.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eaef905-d06d-4b67-9598-a7fcbeaa6593",
   "metadata": {},
   "source": [
    "How it works to get $\\hat \\beta$?\n",
    "\n",
    "\n",
    "\\begin{align}\n",
    "  (X'X) \\hat \\beta &=  X'y  \\\\\n",
    "  (R'Q'QR) \\hat \\beta &=  R'Q'y  \\\\\n",
    "  (R'R) \\hat \\beta &=  R'Q'y  \\\\\n",
    "  R \\hat \\beta &=  Q'y  \n",
    "\\end{align}\n",
    "\n",
    "- Solve by back substitution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53b73431-4212-4464-9823-924df070d8c2",
   "metadata": {},
   "source": [
    "1. QR factorization: $X'X = QR$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98ecf0cd-472b-4f2c-9beb-ccd99b522c20",
   "metadata": {},
   "outputs": [],
   "source": [
    "XtX<-t(X)%*%X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ecd2e05-949a-4fac-9f90-4ee0d27087ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "QR<-qr(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa149ee1-57ae-4efa-8abd-3dd94a2bc1de",
   "metadata": {},
   "outputs": [],
   "source": [
    "R<- qr.R(QR)\n",
    "R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30673173-8d55-4876-a084-89bc2bab01ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q<- qr.Q(QR)\n",
    "Q"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6085a7b-7c1b-46ab-9a32-efa6d9b93c90",
   "metadata": {},
   "source": [
    "2. Calculate $Q'y$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b04c616a-1f77-413a-82ec-75eaccadfe41",
   "metadata": {},
   "outputs": [],
   "source": [
    "t(Q)%*%y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7324646-bdaa-492d-8ab9-52e608f9de7d",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "3. Solve:\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "-1.73 & -20.78 \\\\\n",
    "0 & -5.65\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "\\beta_0 \\\\\n",
    "\\beta_1\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "-15.87 \\\\\n",
    "-5.30\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Solution: $(\\beta_1, \\beta_2) = (3.5, -0.5)$.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2243cde9-ef12-41ab-9c54-4959613cfa99",
   "metadata": {},
   "outputs": [],
   "source": [
    "beta1<-  -5.303301/-5.656854\n",
    "beta1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a70cbf74-8a36-4135-8be1-e5435ea72d77",
   "metadata": {},
   "outputs": [],
   "source": [
    "beta0<- (-15.877132 -  (-20.784610*beta1)) /-1.732051\n",
    "beta0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cb27a3c-25b8-4190-97b0-09f7da8fbf1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "lm.fit(X,y)$coefficients"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f76e0c4-3644-45ce-bba4-1f876fc7ddec",
   "metadata": {},
   "source": [
    "### Householder transformation (Extra)\n",
    "\n",
    "\n",
    "There are many ways to implement the QR decomposition, like the Gram-Schmidt process and the Household transformation. \n",
    "In R, the QR decomposition used in functions like `lm()` is implemented via Householder reflections, a standard and numerically stable algorithm for computing the QR decomposition implemented using the [LAPACK library](https://www.netlib.org/lapack/).\n",
    "\n",
    "Householder reflections are much more stable than the Gram-Schmidt process, which can accumulate rounding errors, and the algorithms are quite efficient for moderate data sets.\n",
    "\n",
    "To illustrate the **Householder transformation** for the QR decomposition in your linear regression example, we can explicitly perform the steps of the algorithm. The goal is to decompose $ \\mathbf{X} $ into $ \\mathbf{Q} $ and $ \\mathbf{R} $, where $ \\mathbf{Q} $ is orthogonal and $ \\mathbf{R} $ is upper triangular. The algorithm for the QR decomposition can be summarized as:\n",
    "\n",
    "**Input**: Matrix $ \\mathbf{X} $ of size $ n \\times k $.  \n",
    "**Output**: Matrices $ \\mathbf{Q} $ (orthogonal) and $ \\mathbf{R} $ (upper triangular).\n",
    "\n",
    "\n",
    "##### **Steps**\n",
    "1. **Initialize**:\n",
    "   - Let $ \\mathbf{Q} = \\mathbf{I}_n $ (identity matrix of size $ n \\times n $).\n",
    "   - Let $ \\mathbf{R} = \\mathbf{X} $.\n",
    "\n",
    "2. **Iterate over each column of $ \\mathbf{X} $**:\n",
    "   - For $ j = 1, 2, \\dots, k $:\n",
    "     1. Extract the column $ \\mathbf{r}_j = \\mathbf{R}[j:n, j] $ (the portion below and including the diagonal).\n",
    "     2. Compute the **norm** of $ \\mathbf{r}_j $:  \n",
    "        $ \\text{norm} = \\|\\mathbf{r}_j\\|_2 = \\sqrt{\\sum (\\mathbf{r}_j^2)} $.\n",
    "     3. Construct the **reflection vector** $ \\mathbf{v} $:\n",
    "        - $ \\mathbf{v} = \\mathbf{r}_j $\n",
    "        - Modify the first element of $ \\mathbf{v} $:  \n",
    "          $ \\mathbf{v}[1] = \\mathbf{r}_j[1] + \\text{sign}(\\mathbf{r}_j[1]) \\cdot \\text{norm} $.\n",
    "        - Normalize $ \\mathbf{v} $:  \n",
    "          $ \\mathbf{v} = \\mathbf{v} / \\|\\mathbf{v}\\|_2 $.\n",
    "     4. Form the **Householder matrix** $ \\mathbf{H}_j $:\n",
    "        - $ \\mathbf{H}_j = \\mathbf{I} - 2 \\mathbf{v} \\mathbf{v}^\\top $.\n",
    "        - Expand $ \\mathbf{H}_j $ to size $ n \\times n $ by applying it to rows $ j $ to $ n $ only.\n",
    "     5. Apply $ \\mathbf{H}_j $ to $ \\mathbf{R} $:\n",
    "        - $ \\mathbf{R} = \\mathbf{H}_j \\mathbf{R} $.\n",
    "     6. Accumulate $ \\mathbf{Q} $:\n",
    "        - $ \\mathbf{Q} = \\mathbf{Q} \\cdot \\mathbf{H}_j^\\top $.\n",
    "\n",
    "3. **Finalize**:\n",
    "   - The resulting $ \\mathbf{Q} $ is orthogonal, and $ \\mathbf{R} $ is upper triangular.\n",
    "\n",
    "Here's how we can approach this in R, step by step:\n",
    "\n",
    "##### Step 1: Define the Design Matrix\n",
    "\n",
    "We take our design matrix $\\mathbf{X}$ \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "223ae876-119b-407f-b4da-b984251c5ba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55a5c782-dff0-43af-a97e-c8fee15e1711",
   "metadata": {},
   "source": [
    "##### Step 2: Apply the Householder Transformation\n",
    "The Householder transformation is applied to zero out the sub-diagonal entries of the first column of $ \\mathbf{X}$ , then the second column, and so on.\n",
    "\n",
    "\n",
    "\n",
    "###### First Reflection\n",
    "1. Define the first column $ \\mathbf{x}_1 $ of $ \\mathbf{X}$.\n",
    "2. Compute the reflection vector $ \\mathbf{v} $ and construct the Householder matrix $ \\mathbf{H}_1 $.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "402a84c7-546b-4cc0-90d8-9d2a11d46849",
   "metadata": {},
   "outputs": [],
   "source": [
    "householder <- function(x) {\n",
    "  # Compute the norm of x\n",
    "  norm_x <- sqrt(sum(x^2))\n",
    "  \n",
    "  # Define the reflection vector v\n",
    "  v <- x\n",
    "  v[1] <- x[1] + sign(x[1]) * norm_x\n",
    "  \n",
    "  # Normalize v\n",
    "  v <- v / sqrt(sum(v^2))\n",
    "  \n",
    "  # Construct the Householder matrix\n",
    "  H <- diag(length(x)) - 2 * v %*% t(v)\n",
    "  \n",
    "  return(H)\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aeb4a38-c9d5-485b-8a21-99d84dd4923d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First column of X\n",
    "x1 <- X[, 1]\n",
    "\n",
    "# Apply Householder transformation for the first column\n",
    "H1 <- householder(x1)\n",
    "\n",
    "H1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67af106f-229e-4b72-adb4-d24513e64675",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform X\n",
    "X1 <- H1 %*% X\n",
    "X1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a12a530e-9aa9-49be-a940-5f0b5db7f68e",
   "metadata": {},
   "source": [
    "#####  Second Reflection\n",
    "We now apply a second Householder transformation to the second column of $ \\mathbf{X}_1$, ignoring the first row (already triangular).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e78bc95e-a98a-466c-9cfc-26ed6155a366",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the second column below the diagonal\n",
    "x2 <- X1[2:3, 2]\n",
    "\n",
    "# Apply Householder transformation\n",
    "H2_partial <- householder(x2)\n",
    "\n",
    "# Expand H2 to be a full matrix\n",
    "H2 <- diag(3)\n",
    "H2[2:3, 2:3] <- H2_partial\n",
    "\n",
    "H2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "943d09cc-df82-4a4f-9a5a-42e60543785e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform X1\n",
    "X2 <- H2 %*% X1\n",
    "X2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5635b842-f08e-44e2-8a2a-df7d4026e40e",
   "metadata": {},
   "source": [
    "At this stage, $\\mathbf{X}_2$ is fully upper triangular.\n",
    "\n",
    "##### Step 3: Combine Householder Matrices\n",
    "The overall $ \\mathbf{Q} $ matrix is the product of the Householder matrices:\n",
    "$$\n",
    "\\mathbf{Q} = H_1^\\top H_2^\\top\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ec7abe4-13f9-49e2-a9e7-dc7f92d2d2fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "Qhoush <- t(H2) %*% t(H1)\n",
    "Qhoush"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09d86e9f-1555-491a-be8e-cfdbc1dfd62b",
   "metadata": {},
   "outputs": [],
   "source": [
    "Rhouse<-X2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a98c3ea-2640-4cfc-860b-8260dd049b7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "round(Rhouse,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d552a72-4a01-4015-8d66-11b4cdfa8b5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "round(R,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9455ab93-887e-469b-b68e-9e5c5d9b2973",
   "metadata": {},
   "source": [
    "**Why the Last Row of $ \\mathbf{R} $ in the Householder transformation is Zero?**\n",
    "\n",
    "The matrix $ \\mathbf{R} $ is the result of applying the Householder transformations to $ \\mathbf{X} $. If $ \\mathbf{X} $ has more rows ($ n $) than columns ($ k $), the matrix $ \\mathbf{R} $ will have a size $ n \\times k $, but:\n",
    "\n",
    "1. **Triangular Shape**: $ \\mathbf{R} $ is upper triangular by definition.\n",
    "   - The first $ k $ rows of $ \\mathbf{R} $ contain meaningful values.\n",
    "   - The remaining $ n - k $ rows (if $ n > k $) are all zeros because the decomposition process eliminates entries below the main diagonal for each column of $ \\mathbf{X} $.\n",
    "\n",
    "2. **Overdetermined Systems**: In regression, $ n > k $ is common because there are more observations than predictors.\n",
    "   - After applying the Householder reflections, the additional rows beyond the $ k $-th row become zeros due to the nature of the decomposition.\n",
    "\n",
    "\n",
    "**What R's `qr()` Function Does**\n",
    "In R, the `qr()` function is designed to return a compact representation of $ \\mathbf{R} $:\n",
    "- **Truncated $ \\mathbf{R} $**: The function returns only the first $ k $ rows of $ \\mathbf{R} $, as the remaining $ n - k $ rows are guaranteed to be zeros and contain no additional information.\n",
    "- This behavior intentionally saves memory and focuses on the part of $ \\mathbf{R} $ that is meaningful for solving the regression problem.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0f42b97-1b38-45d6-be1f-ede0fda1bc92",
   "metadata": {},
   "source": [
    "## Traditional Computation using SVD Decomposition\n",
    "\n",
    "The **Singular Value Decomposition (SVD)** provides a robust method for solving linear regression problems, particularly when the design \n",
    "matrix $ \\mathbf{X} $ is ill-conditioned or rank-deficient. For a matrix $ \\mathbf{X} $ of size $ n \\times k $, the SVD is:\n",
    "\n",
    "$$\n",
    "\\mathbf{X} = \\mathbf{U} \\mathbf{\\Sigma} \\mathbf{V}^\\top,\n",
    "$$\n",
    "\n",
    "where $ \\mathbf{U} $ ($ n \\times n $) and $ \\mathbf{V} $ ($ k \\times k $) are orthogonal matrices, and $ \\mathbf{\\Sigma} $ ($ n \\times k $) is a diagonal matrix containing the singular values of $ \\mathbf{X} $, denoted as $ \\sigma_1, \\sigma_2, \\ldots, \\sigma_r $ ($ r $ is the rank of $ \\mathbf{X} $).\n",
    "\n",
    "To solve the regression $ \\mathbf{y} = \\mathbf{X} \\mathbf{\\beta} + \\mathbf{\\epsilon} $, the coefficients $ \\mathbf{\\beta} $ are estimated using the pseudoinverse of $ \\mathbf{X} $:\n",
    "\n",
    "$$\n",
    "\\mathbf{\\beta} = (\\mathbf{X}^\\top \\mathbf{X})^{-1} \\mathbf{X}^\\top \\mathbf{y} = \\mathbf{V} \\mathbf{\\Sigma}^+ \\mathbf{U}^\\top \\mathbf{y},\n",
    "$$\n",
    "\n",
    "where $ \\mathbf{\\Sigma}^+ $ is the pseudoinverse of $ \\mathbf{\\Sigma} $, obtained by inverting the nonzero singular values and transposing the matrix. This method is numerically stable and handles cases where $ \\mathbf{X} $ is not full rank.\n",
    "\n",
    "This is the way that is solved in `Python` by [sklearn](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html) that [LAPACK](https://docs.scipy.org/doc/scipy/reference/generated/scipy.linalg.lstsq.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4eb5b53-9099-4fcf-aa6f-8555ea9f376d",
   "metadata": {},
   "source": [
    "## Big Data and Machine Learning Way: Gradient Descent\n",
    "\n",
    "<div>\n",
    "    <img src=\"figs/meme_gradient_descent.png\"  style=\"width: 300px;\" />\n",
    "</div>\n",
    "\n",
    "- Gradient Descent is a generic optimization algorithm that finds optimal solutions to various problems.\n",
    "- The general idea is to tweak parameters iteratively to minimize a loss function:\n",
    "\n",
    "$$\n",
    "\\text{min}_f E[L(y_i, f(\\boldsymbol{X_i}))]\n",
    "$$\n",
    "\n",
    "### Computing OLS Coefficients\n",
    "\n",
    "The problem is to estimate the coefficients of vector $\\beta$ that minimize the objective function:\n",
    "\n",
    "$$\n",
    "\\text{argmin}_\\beta \\sum_{i=1}^{n} \\frac{1}{n} \\left(y_i - \\beta_0 + \\sum_{k=1}^K X_k \\beta_k\\right)^2\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "#### Intuition: Loss Function (1D and 2D)\n",
    "\n",
    "##### **1D Loss Function:**\n",
    "\n",
    "<div >\n",
    "<img src = \"figs/reg0.png\"   style=\"width: 800px;\"  />\n",
    "</div>\n",
    "\n",
    "#####  **2D Loss Function:**\n",
    "\n",
    "\n",
    "<div >\n",
    "<img src = \"figs/ols1.png\"  style=\"width: 800px;\" />\n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61240628-1045-4593-9a77-7e6dee803412",
   "metadata": {},
   "source": [
    "#### The algorithm\n",
    "\n",
    "<div >\n",
    "<img src = \"figs/step_size1.png\"  style=\"width: 800px;\" />\n",
    "</div>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "1. Randomly pick starting values for the parameters.\n",
    "2. Compute the gradient of the objective function at the current parameter values using all observations from the sample.\n",
    "3. Update the parameters.\n",
    "4. Repeat from step 2 until a fixed number of iterations or convergence.\n",
    "\n",
    "More formally,\n",
    "\n",
    "- At a point $\\beta \\in \\mathbb{R}^k$, at step $j$, Gradient Descent moves in a direction $\\delta \\beta$ such that:\n",
    "\n",
    "$$\n",
    "L(\\beta^{(j)} + \\delta \\beta) < L(\\beta^{(j)})\n",
    "$$\n",
    "\n",
    "- Choice of $\\delta \\beta$:\n",
    "\n",
    "$$\\delta \\beta = -\\epsilon \\nabla_\\beta L(\\beta^{(j)})$$\n",
    "\n",
    "-   replacing\n",
    "\n",
    "$$\n",
    "\\beta^{(j+1)} = \\beta^{(j)} - \\epsilon \\nabla_\\beta L(\\beta^{(j)})\n",
    "$$\n",
    "\n",
    "- Define the learning step $\\epsilon$ (*more on this below*).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8e9850f-f568-4df3-bc88-1d0ec29cae97",
   "metadata": {},
   "source": [
    "In our example, OLS example\n",
    "\n",
    "\n",
    "$$\n",
    "L(\\alpha,\\beta)=\\frac{1}{n}\\sum_{i=1}^{n}(y_{i}-(\\alpha+\\beta x_{i}))^{2} \\nonumber\n",
    "$$\n",
    "\n",
    "The Gradient\n",
    "\n",
    "\\begin{align}\n",
    "\\nabla L(\\alpha,\\beta)=\\left(\\begin{array}{c}\n",
    "\\frac{\\partial L}{\\partial\\alpha}\\\\\n",
    "\\frac{\\partial L}{\\partial\\beta}\n",
    "\\end{array}\\right)=\\left(\\begin{array}{c}\n",
    "-\\frac{2}{n}\\sum_{i=1}^{n}(y_{i}-\\alpha-\\beta x_{i})\\\\\n",
    "-\\frac{2}{n}\\sum_{i=1}^{n}x_{i}(y_{i}-\\alpha-\\beta x_{i})\n",
    "\\end{array}\\right)  \\nonumber\n",
    "\\end{align}\n",
    "\n",
    "Updating\n",
    "\\begin{align}\n",
    "\\alpha^{(j+1)} &=\\alpha^{(j)}-\\epsilon\\frac{\\partial L}{\\partial\\alpha} \\nonumber \\\\\n",
    "\\beta^{(j+1)} &= \\beta^{(j)}-\\epsilon\\frac{\\partial L}{\\partial\\beta} \\nonumber\n",
    "\\end{align}\n",
    "\n",
    "\n",
    "\n",
    "Start with an initial guess: $\\alpha=-1;\\beta=2$ , and a learning rate ($\\epsilon=0.005$). Then we have\n",
    "\n",
    "\\begin{align}\n",
    "\\alpha^{j=2} &=(-1)-0.005\\left(-2/3\\times\\left((5-(-1)-2\\times8)+(10-(-1)-2\\times12\\right)+(12.5-(-1)-2\\times16)\\right) \\nonumber \\\\\n",
    "\\beta^{j=2} &=2+0.005\\left(-2/3\\times\\left(8(5-(-1)-2\\times8)+12(10-(-1)-2\\times12\\right)+16(12.5-(-1)-2\\times16)\\right) \\nonumber \\\\\n",
    "\\alpha^{j=2}&=-1.1384 \\nonumber \\\\\n",
    "\\beta^{j=2} &=0.2266 \\nonumber\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25bcac4b-0310-4603-866f-4ed7bfa9b718",
   "metadata": {},
   "outputs": [],
   "source": [
    "gradiente_alpha<-function(alpha,beta){ -(2/3)*((5-alpha-beta*8)+(10-alpha-beta*12)+(12.5-alpha-beta*16))}\n",
    "gradiente_beta<-function(alpha,beta){ -(2/3)*(8*(5-alpha-beta*8)+12*(10-alpha-beta*12)+16*(12.5-alpha-beta*16))}\n",
    "\n",
    "\n",
    "# Empty vector\n",
    "alpha<-list()\n",
    "beta<-list()\n",
    "\n",
    "# First Values\n",
    "alpha[[1]]<- -1\n",
    "beta[[1]]<- 2\n",
    "\n",
    "# Learning Rate\n",
    "epsilon<-0.005\n",
    "\n",
    "# Second iteration\n",
    "j<-2\n",
    "\n",
    "alpha[[j]]<-alpha[[j-1]]-epsilon*gradiente_alpha(alpha[[j-1]],beta[[j-1]])\n",
    "beta[[j]]<- beta[[j-1]]-epsilon*gradiente_beta(alpha[[j-1]],beta[[j-1]])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a2a1e04-cabe-44e6-aeca-13f99d6a8258",
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha[[j]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9479482-18fb-4116-8c4e-20aaac068818",
   "metadata": {},
   "outputs": [],
   "source": [
    "beta[[j]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80e76b37-9050-4731-ab55-b2177248bd8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ggplot(dta,aes(x=educ,y=lnwage)) +\n",
    "  geom_point(alpha=1,size=4) +\n",
    "  theme_bw()  +\n",
    "  xlab(\"Education\") +\n",
    "  ylab(\"Ln(Wage)\") +\n",
    "  xlim(0,20) +\n",
    "  ylim(-5,20) +\n",
    "  geom_abline(intercept = alpha[[j]], slope = beta[[j]],linewidth=3, col=\"red\",lty=\"dashed\") +\n",
    "  theme(legend.position = \"none\",\n",
    "        panel.grid.major = element_blank(),\n",
    "        panel.grid.minor = element_blank(),\n",
    "        text = element_text(size=20)\n",
    "  ) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c08e00f0-06ce-4092-ae64-16ed54f72833",
   "metadata": {},
   "outputs": [],
   "source": [
    "j<-3\n",
    "\n",
    "alpha[[j]]<-alpha[[j-1]]-epsilon*gradiente_alpha(alpha[[j-1]],beta[[j-1]])\n",
    "beta[[j]]<- beta[[j-1]]-epsilon*gradiente_beta(alpha[[j-1]],beta[[j-1]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02ffaa63-3fc4-4f0e-bcd8-ba487045a1fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8115b87b-ddfa-4dc5-9244-d77ce0ffdfed",
   "metadata": {},
   "outputs": [],
   "source": [
    "beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3afd92e2-3359-4add-bd60-0456d88aa325",
   "metadata": {},
   "outputs": [],
   "source": [
    "ggplot(dta,aes(x=educ,y=lnwage)) +\n",
    "  geom_point(alpha=1,size=4) +\n",
    "  theme_bw()  +\n",
    "  xlab(\"Education\") +\n",
    "  ylab(\"Ln(Wage)\") +\n",
    "  xlim(0,20) +\n",
    "  ylim(-5,20) +\n",
    "  geom_abline(intercept = alpha[[j]], slope = beta[[j]],linewidth=3, col=\"red\",lty=\"dashed\") +\n",
    "  theme(legend.position = \"none\",\n",
    "        panel.grid.major = element_blank(),\n",
    "        panel.grid.minor = element_blank(),\n",
    "        text = element_text(size=20)\n",
    "  ) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "096d39f9-ff0f-46a6-a967-9c9e6631dd3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "i<-2\n",
    "tol<-0.000001\n",
    "\n",
    "difff<-2\n",
    "\n",
    "Loss<-function(alpha,beta){ 1/3*((5-alpha-beta*8)+(10-alpha-beta*12)+(12.5-alpha-beta*16))}\n",
    "\n",
    "while( (difff>tol)==TRUE ){\n",
    "  \n",
    "  alpha[[i+1]]<- alpha[[i]]- epsilon*gradiente_alpha(alpha[[i]],beta[[i]])\n",
    "  beta[[i+1]] <- beta[[i]] - epsilon*gradiente_beta(alpha[[i]],beta[[i]])\n",
    "  \n",
    "  difff<-abs(Loss(alpha[[i]],beta[[i]])-Loss(alpha[[i-1]],beta[[i-1]]))\n",
    "  \n",
    "  i<-i+1\n",
    "  print(i)\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "535920da-dceb-42f3-b6e9-4e623f59ce01",
   "metadata": {},
   "outputs": [],
   "source": [
    "ggplot(dta,aes(x=educ,y=lnwage)) +\n",
    "  geom_point(alpha=1,size=4) +\n",
    "  theme_bw()  +\n",
    "  xlab(\"Education\") +\n",
    "  ylab(\"Ln(Wage)\") +\n",
    "  xlim(0,20) +\n",
    "  ylim(-5,20) +\n",
    "  geom_abline(intercept = alpha[[i]], slope = beta[[i]],linewidth=3, col=\"red\",lty=\"dashed\") +\n",
    "  #geom_abline(intercept = lm.fit(X,y)$coefficients[1], slope = lm.fit(X,y)$coefficients[2],linewidth=3, col=\"blue\",lty=\"dashed\") +\n",
    "  theme(legend.position = \"none\",\n",
    "        panel.grid.major = element_blank(),\n",
    "        panel.grid.minor = element_blank(),\n",
    "        text = element_text(size=20)\n",
    "  ) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0b2de58-6f19-418c-90f1-336f5db6311d",
   "metadata": {},
   "source": [
    "### The learning rate\n",
    "\n",
    "We can choose $\\epsilon$ in several different ways:\n",
    "\n",
    " - Set $\\epsilon$ to a small constant. (like before)\n",
    " \n",
    "\n",
    "\n",
    "<div >\n",
    "<img src = \"figs/step_size2.png\"  style=\"width: 800px;\"/>\n",
    "</div>\n",
    "\n",
    "- Use varying learning rates (*more on this later*)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43937ead-cac7-4fa3-bf56-e7a01059b345",
   "metadata": {},
   "source": [
    "## Computational Complexity\n",
    "\n",
    "| **Method**             | **Computational Complexity**                     | **Description**                                                                                     | **Large $k$** | **Large $n$** | **Hyperparameters** |\n",
    "|-------------------------|--------------------------------------------------|-----------------------------------------------------------------------------------------------------|----------------|----------------|----------------------|\n",
    "| **Inverting $ \\mathbf{X}'\\mathbf{X} $** | $ \\mathcal{O}(nk^2 + k^3) $                    | Direct computation using $ \\mathbf{\\beta} = (\\mathbf{X}^\\top \\mathbf{X})^{-1} \\mathbf{X}^\\top \\mathbf{y} $. Requires inversion of $ \\mathbf{X}^\\top \\mathbf{X} $, which is costly for large $k$. | Slow               | Slow               | 0                    |\n",
    "| **QR Decomposition**   | $ \\mathcal{O}(nk^2) $                          | Used in R's `lm()`. Solves $ \\mathbf{X}^\\top \\mathbf{X} \\mathbf{\\beta} = \\mathbf{X}^\\top \\mathbf{y} $ via QR decomposition. Efficient for moderate $k$. | Fast               | Slow               | 0                    |\n",
    "| **SVD**                | $ \\mathcal{O}(nk^2 + k^3) $                    | Used in Python's `scikit-learn`. Computes $ \\mathbf{X} = \\mathbf{U} \\mathbf{\\Sigma} \\mathbf{V}^\\top $. Provides numerical stability. Suitable for ill-conditioned matrices. | Fast               | Slow               | 0                    |\n",
    "| **Gradient Descent**   | $ \\mathcal{O}(n k \\cdot \\text{iter}) $          | Iterative method. $ \\text{iter} $ is the number of iterations required for convergence. Efficient for very large $n$, $k$. | Slow               | Fast               | 2                    |\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.4.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
