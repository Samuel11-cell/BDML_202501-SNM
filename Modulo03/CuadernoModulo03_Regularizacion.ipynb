{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a952ad54-2218-4382-b2eb-d401d349f764",
   "metadata": {},
   "source": [
    "<div >\n",
    "<img src = \"../banner.jpg\" />\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96eb7275",
   "metadata": {},
   "source": [
    "<a target=\"_blank\" href=\"https://colab.research.google.com/github/ignaciomsarmiento/BDML_202302/blob/main/Lecture04/Notebook_SS04_Ridge.ipynb\">\n",
    "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
    "</a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7e57de4",
   "metadata": {},
   "source": [
    "\n",
    "# Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "303a3dcd",
   "metadata": {},
   "source": [
    "## Predicting Wages\n",
    "\n",
    "Our objective today is to construct a model of individual wages\n",
    "\n",
    "$$\n",
    "w = f(X) + u \n",
    "$$\n",
    "\n",
    "where w is the  wage, and X is a matrix that includes potential explanatory variables/predictors. In this tutorial set, we will focus on a linear model of the form\n",
    "\n",
    "\\begin{align}\n",
    " ln(w) & = \\beta_0 + \\beta_1 X_1 + \\dots + \\beta_p X_p  + u \n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "312ba2f4",
   "metadata": {},
   "source": [
    "were $ln(w)$ is the logarithm of the wage.\n",
    "\n",
    "To illustrate I'm going to use a sample of the NLSY97. The NLSY97 is  a nationally representative sample of 8,984 men and women born during the years 1980 through 1984 and living in the United States at the time of the initial survey in 1997.  Participants were ages 12 to 16 as of December 31, 1996.  Interviews were conducted annually from 1997 to 2011 and biennially since then.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fdab5a6",
   "metadata": {},
   "source": [
    "Let's load the packages and the data set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6979f85e",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "#install.packages(\"pacman\") #for google colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e94ec2b-ab94-4ec8-8512-e370d2f22a56",
   "metadata": {
    "tags": [
     "remove_cell"
    ],
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "#packages\n",
    "require(\"pacman\")\n",
    "p_load(\"tidyverse\",\"stargazer\")\n",
    "\n",
    "nlsy <- read_csv('https://raw.githubusercontent.com/ignaciomsarmiento/datasets/main/nlsy97.csv')\n",
    "\n",
    "nlsy <- nlsy  %>%   drop_na(educ) #drops NAs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49d73d77",
   "metadata": {},
   "source": [
    "We want to construct a model that predicts well out of sample, and we have potentially 994 regressors. We are going to regularize this regression using the `glmnet` package.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b5df971",
   "metadata": {},
   "source": [
    "## Glmnet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14e93ba3",
   "metadata": {},
   "source": [
    "To apply a regularized model, we can use the `glmnet::glmnet()` function. `glmnet` solves the problem\n",
    "\n",
    "$$\n",
    "\\min_{\\beta_0,\\beta} \\frac{1}{N} \\sum_{i=1}^{N} w_i l(y_i,\\beta_0+\\beta' x_i) + \\lambda\\left[(1-\\alpha)\\frac{1}{2}\\|\\beta\\|_2^2 + \\alpha \\|\\beta\\|_1\\right],\n",
    "$$\n",
    "\n",
    "where $l(y_i,\\beta_0+\\beta^T x_i)$ in the regression case is $\\frac{1}{2}(y_i-\\beta_0-\\beta' x_i)$\n",
    "\n",
    "The `alpha` parameter tells `glmnet` to perform a ridge (`alpha` = 0), lasso (`alpha` = 1), or elastic net (0 < `alpha` < 1) model. \n",
    "\n",
    "By default, `glmnet` will do two things that you should know:\n",
    "\n",
    "1.  By default, `glmnet` automatically standardizes your features. If you standardize your predictors prior to glmnet you can turn this argument off with `standardize = FALSE`.\n",
    "\n",
    "2. The regularization path is computed at a grid of values (on the log scale) for the regularization parameter $\\lambda$. The algorithm is extremely fast!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48194268",
   "metadata": {},
   "source": [
    "`glmnet` has some drawbacks, the main one is that we need to specify the arguments in terms of matrices and vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a0c83b7",
   "metadata": {},
   "source": [
    "`caret`, in contrast, streamlines the process of creating predictive models by providing a uniform interface for predictive models, which, among other things, allows for specifying formulas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7ba08aa",
   "metadata": {},
   "source": [
    "## Ridge\n",
    "\n",
    "We first illustrate ridge regression, which can be fit using `glmnet()` with alpha = 0 and seeks to minimize\n",
    "\n",
    "$$\n",
    "\\frac{1}{2n}\\sum_{i=1}^{n} \\left( y_i - \\beta_0 - \\sum_{j=1}^{p} \\beta_j x_{ij}    \\right) ^ 2 + \\lambda \\frac{1}{2}\\sum_{j=1}^{p} \\beta_j^2 .\n",
    "$$\n",
    "\n",
    "Notice that the intercept is not penalized. Why?\n",
    "\n",
    "\n",
    "Ridge penalizes the squares  of the coefficients. As a result, ridge shrinks coefficients toward zero, but not all the way.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2765df72",
   "metadata": {},
   "source": [
    "To understand the mechanics, we are going to focus on a small subset of variables:\n",
    "\n",
    "1. **educ (Education Level)**  \n",
    "   - This typically refers to the respondent's **highest level of education completed**.  \n",
    "   - It is measured in **years of schooling** \n",
    " \n",
    "2. **mom_educ (Mother’s Education Level)**\n",
    "\n",
    "- This represents the highest level of education completed by the respondent’s mother.\n",
    "- Like \"educ,\" it can be measured in years of schooling \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12c5e4bb",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "p_load(\"glmnet\")\n",
    "\n",
    "#Vector that needs predicting\n",
    "y <- nlsy$lnw_2016\n",
    "\n",
    "# Matrix of predictors \n",
    "Xsmall <- as.matrix(nlsy  %>% select(educ,mom_educ))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9428cd34-a465-4253-8911-1eb70109939b",
   "metadata": {},
   "source": [
    "Let's run the ridge regression (we need to set the parameter `alpha` to zero)\n",
    "\n",
    "We solve:\n",
    "\n",
    "$$\n",
    "\\min_{\\beta_0,\\beta_1,\\beta_2} \\frac{1}{N} \\sum_{i=1}^{n} \\left( log(wage)_i - \\beta_0 -  \\beta_1 Educ_i - \\beta_2 MomEduc_i   \\right) ^ 2 + \\lambda \\frac{1}{2} \\left(\\beta_1^2+ \\beta_2^2 \\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7876f0c1",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "ridge1 <- glmnet(\n",
    "  x = Xsmall,\n",
    "  y = y,\n",
    "  lambda=1,\n",
    "  alpha = 0 #ridge\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "997324f9",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "coef(ridge1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d855810d-a123-485a-aa13-65925721a08b",
   "metadata": {},
   "source": [
    "Are these shrunken relative to OLS?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5472966-9ba7-401b-a990-387334d4de26",
   "metadata": {},
   "outputs": [],
   "source": [
    "stargazer(lm(y~Xsmall),type=\"text\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f175775-868e-4f81-ab23-407fa7d43c3c",
   "metadata": {},
   "source": [
    "H.W. Why the intercepts are not equal? Tip: Think on how the intercept is computed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "745b8db6-42be-4cb0-921f-597568bcd68b",
   "metadata": {},
   "source": [
    "### Regularization Paths"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46568ae7",
   "metadata": {},
   "source": [
    "Let's see the regularization path, that shows how much the coefficients are penalized for different values of $\\lambda$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e8dc2f3",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "ridge2 <- glmnet(\n",
    "  x = Xsmall,\n",
    "  y = y,\n",
    "  alpha = 0 #ridge\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33d7f73c",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "plot(ridge2, xvar = \"lambda\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "604622c3",
   "metadata": {},
   "source": [
    "Notice none of the coefficients are forced to be zero, although they get close to it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a84285ef",
   "metadata": {},
   "source": [
    "#### All the predictors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6bc55f3",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Matrix of predictors (all but lnw_2016)\n",
    "X <- as.matrix(nlsy  %>% select(-lnw_2016))\n",
    "\n",
    "ridge_all <- glmnet(\n",
    "  x = X,\n",
    "  y = y,\n",
    "  alpha = 0 #ridge\n",
    ")\n",
    "\n",
    "plot(ridge_all, xvar = \"lambda\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "610e187b",
   "metadata": {},
   "source": [
    "\n",
    "The *regularization path*, which illustrates how the model’s coefficients change as we vary the regularization strength $\\lambda$. Specifically:\n",
    "\n",
    "1. **Penalizing Coefficients:**  \n",
    "   When $\\lambda$ is large, the penalty on the size of the coefficients is strong, pushing most coefficients toward zero (or shrinking them substantially). This helps prevent overfitting by limiting how large the coefficients are.\n",
    "\n",
    "2. **Relaxing the Penalty:**  \n",
    "   As $\\lambda$ decreases, the penalty weakens. Coefficients that were previously shrunk toward zero may increase in magnitude if they improve the model’s fit to the data. Consequently, the model becomes more flexible and potentially more prone to overfitting.\n",
    "\n",
    "3. **Visualizing the Path:**  \n",
    "   We can plot each coefficient across a range of $\\lambda$ values. Each line on the plot corresponds to a coefficient, showing how it changes from near-zero (with high $\\lambda$) to larger values (with lower $\\lambda$).\n",
    "\n",
    "Overall, analyzing this path provides insight into the trade-off between regularization (controlling overfitting) and model complexity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46bdff4d",
   "metadata": {},
   "source": [
    "### Scale Equivariance\n",
    "\n",
    "Since regularized methods apply a penalty to the coefficients, we need to ensure our coefficients are on a common scale. If not, then predictors with naturally larger values  will be penalized more than predictors with naturally smaller values.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "552bd6f6",
   "metadata": {},
   "source": [
    "Let's run the ridge regression (we need to set the parameter `alpha` to zero)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7d962d5",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "ridge <- glmnet(\n",
    "  x = Xsmall,\n",
    "  y = y,\n",
    "  alpha = 0, #ridge\n",
    "  lambda=20,\n",
    "  standardize=FALSE,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f5c15e1",
   "metadata": {},
   "source": [
    "Let's see the coefficients we obtained\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea487ab0",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "coef(ridge)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e6d1503",
   "metadata": {},
   "source": [
    "Compare to OLS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d58c8e2",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "ols<-lm(y~Xsmall)\n",
    "stargazer(ols,type=\"text\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b170493",
   "metadata": {},
   "source": [
    "#### What happens if we change the scale for mother's education?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85450b47",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "Xsmall[,2]<-Xsmall[,2]/10 #divide by 10, put it in decades"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48a876d2",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "ridge_10 <- glmnet(\n",
    "  x = Xsmall,\n",
    "  y = y,\n",
    "  alpha = 0, #ridge\n",
    "  lambda=20,\n",
    "  standardize=FALSE,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d1fadfa",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "coef(ridge_10)[3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b56d0c3d-e964-40b6-bf15-ae66b941957b",
   "metadata": {},
   "source": [
    "The original in years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be90c090-5aed-486c-8889-f6884b66c1e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "coef(ridge)[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ebe89dc",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "ols_10<-lm(y~Xsmall)\n",
    "summary(ols_10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37dbc4c9",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "ols_10$coefficients[3]/10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c23df331-7b9e-4cab-be04-ae6e44ce0201",
   "metadata": {},
   "outputs": [],
   "source": [
    "ols$coefficients[3] #the original"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74261278-b2ce-4b51-b9cc-636e4c192610",
   "metadata": {},
   "source": [
    "Does it work if we reescale the ridge coefficient?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bd400e7-0553-4350-af45-f3e0f7b0decd",
   "metadata": {},
   "outputs": [],
   "source": [
    "coef(ridge_10)[3]/10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0203a5c-4420-4159-ae15-e3dda8fed2fd",
   "metadata": {},
   "source": [
    "#### A Bit of intuition\n",
    "\n",
    "$$\n",
    "\\tilde{MomEduc}_i = \\frac{MomEduc_i}{10}\n",
    "$$\n",
    "\n",
    "Now the model becomes:\n",
    "\n",
    "$$\n",
    "log(wage)_i = \\beta_0 -  \\beta_1 Educ_i - \\tilde{\\beta}_2 \\tilde{MomEduc}_i + u_i\n",
    "$$\n",
    "\n",
    "where $\\tilde{\\beta}_2=10 \\beta_2$\n",
    "\n",
    "Substituting into the ridge penalty term:\n",
    "\n",
    "$$\n",
    "\\lambda (\\beta_1^2 + \\beta_2^2) = \\lambda \\left( \\beta_1^2 + \\left(\\frac{\\tilde{\\beta}_2}{10}\\right)^2 \\right)\n",
    "$$\n",
    "\n",
    "which simplifies to:\n",
    "\n",
    "$$\n",
    "\\lambda \\beta_1^2 + \\lambda \\frac{\\tilde{\\beta}_2^2}{100}\n",
    "$$\n",
    "\n",
    "This means that **the penalty on $\\beta_2$ is scaled down by a factor of 100** when the variable is rescaled. In other words:\n",
    "\n",
    "- **Before scaling**, the mother's education variable is shrunken at the same rate as years of education because they are on the same scale.\n",
    "- **After scaling**, the shrinkage is relatively **smaller** because the coefficient is now divided by a factor.\n",
    "\n",
    "\n",
    "Take aways:\n",
    "\n",
    "1. **Magnitude Matters:**  \n",
    "   - The ridge penalty **shrinks large coefficients more** because it treats all variables equally in regularization.\n",
    "   - Thus, if mother's education is measured in years, its coefficient will be **larger** than if measured in decades, leading to a **stronger penalization**.\n",
    "\n",
    "2. **Standardization Helps:**  \n",
    "   - In practice, ridge regression often **standardizes variables** to avoid uneven penalization.\n",
    "   - Standardizing ensures that all variables have a mean of 0 and variance of 1, making ridge penalties apply evenly. Variables now are unitless.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5f16225",
   "metadata": {},
   "source": [
    "### Penalty selection\n",
    "\n",
    "Now we need to discuss how we select $\\lambda$: \n",
    "\n",
    "- Making sure that we have **zero bias** within the sample creates **out-of-sample problems**: **Bias-Variance trade-off**.\n",
    "\n",
    "- We make this trade-off **empirically**.\n",
    "\n",
    "    \\begin{align}\n",
    "            min_{\\beta} E(\\beta) = \\sum_{i=1}^n (y_i-\\beta_0 - x_{i1}\\beta_1 - \\dots - x_{ip}\\beta_p)^2 + \\lambda \\sum_{j=1}^p R(\\beta_j)\n",
    "    \\end{align}\n",
    "\n",
    "- **$\\lambda$ is the price for this trade-off**.\n",
    "\n",
    "How do we choose  $\\lambda$? -> $\\rightarrow$  **Cross-validation**.\n",
    "\n",
    "\n",
    "1. Initialization: Initially, we select a range of $\\lambda$ values. These could be chosen based on prior knowledge, heuristics, or a predefined range.\n",
    "\n",
    "2. Iteration Over Folds:\n",
    "\n",
    "    - Fold 1 as Validation: We train the Ridge regression model on Folds 2-5 (combining them to form the training set) and validate the model on Fold 1. We record the MSE  for each $\\lambda$  value.\n",
    "    - Fold 2 as Validation: Next, we train on Folds 1, 3-5 and validate on Fold 2, recording the MSE  for each $\\lambda$  value.\n",
    "    - We repeat this process, training on four folds and validating on the remaining one, cycling through until each fold has served as the validation set.\n",
    "  \n",
    "\n",
    "<div>\n",
    "<img src=\"../Modulo02/figs_notebook_SS03/fold.png\" width=\"500\"/>\n",
    "</div>\n",
    "\n",
    "\n",
    "- Error Aggregation: For each $\\lambda$   value, we average the MSE across all five folds. \n",
    "\n",
    "- Selection of $\\lambda$ : We select the $\\lambda$  value that yields the lowest MSE. This is the value that we expect to generalize best to unseen data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c13999b-8a28-4125-a496-45d69cbdee4d",
   "metadata": {},
   "source": [
    "Let's do it on all the predictors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ad989ed",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "#Vector that needs predicting\n",
    "y <- nlsy$lnw_2016\n",
    "\n",
    "# Matrix of predictors (all but lnw_2016)\n",
    "X <- as.matrix(nlsy  %>% select(-lnw_2016))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad6ed689-4539-4680-a2ef-94a0fbeecf3d",
   "metadata": {},
   "source": [
    "#### 1. Initialization: Initially, we select a range of $\\lambda$ values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d69365d-07e4-4350-b4eb-2178aaf05d77",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Define lambda sequence\n",
    "lambda_seq <- 10^seq(4, -2, length = 100)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e492c18-4245-4f54-9631-2799a0989234",
   "metadata": {},
   "source": [
    "#### 2. Iteration Over Folds:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ee9de70-8197-4fd2-95f4-54e390a4a46b",
   "metadata": {},
   "source": [
    "##### Create the folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e9d8e9f-600b-41c3-a615-24c8948c3edf",
   "metadata": {},
   "outputs": [],
   "source": [
    "K <- 5  # 5-fold cross-validation\n",
    "\n",
    "# Create folds\n",
    "set.seed(123)  # Ensure reproducibility\n",
    "folds <- sample(rep(1:K, length.out = length(y)))  # Assign randomly 1-5 to each observation.\n",
    "folds "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "948bd10f-c322-465a-89d5-249bf4176a41",
   "metadata": {},
   "source": [
    "# Prepare to loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1548b86-0f3f-4616-8766-043e3651c2b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store errors\n",
    "cv_errors <- matrix(NA, nrow = length(lambda_seq), ncol = K)\n",
    "\n",
    "# Loop over folds\n",
    "for (k in 1:K) {\n",
    "\n",
    "  test_idx <- which(folds == k) # Extract test indices for this fold\n",
    "  \n",
    "  # Training and testing folds\n",
    "  X_train <- X[-test_idx, , drop = FALSE]  # Drop = FALSE prevents dimension reduction issues\n",
    "  y_train <- y[-test_idx]\n",
    "  X_test <- X[test_idx, , drop = FALSE]\n",
    "  y_test <- y[test_idx]\n",
    "  \n",
    "  # Fit ridge regression for all lambdas (100 because that is the sequence we specified)\n",
    "  ridge_model <- glmnet(X_train, y_train, alpha = 0, lambda = lambda_seq)\n",
    "  \n",
    "  # Predict on test set for each lambda\n",
    "  y_pred <- predict(ridge_model, newx = X_test)\n",
    "  \n",
    "  # Compute MSE for each lambda\n",
    "  cv_errors[, k] <- colMeans((y_test - y_pred)^2)\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8da7edb3-f665-4b7f-8a8d-25403610bd3f",
   "metadata": {},
   "source": [
    "##### Error Aggregation: For each $\\lambda$   value, we average the MSE across all five folds. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bedfc91-d1d1-404d-9e38-0b70b3cfc4a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Average errors across folds\n",
    "cv_mse <- rowMeans(cv_errors)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "573c56ae-9ec2-4bc0-8887-66e039eb39e0",
   "metadata": {},
   "source": [
    "##### Selection of $\\lambda$ : We select the $\\lambda$  value that yields the lowest MSE. This is the value that we expect to generalize best to unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c9426f9-c3b4-453f-bb72-cc3e75f1ed3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimal lambda (minimizing the cross-validation error)\n",
    "lambda_opt <- lambda_seq[which.min(cv_mse)]\n",
    "# Print optimal lambda\n",
    "print(lambda_opt)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7be8848a-2cfd-43b8-be72-635c4a931400",
   "metadata": {},
   "source": [
    "##### Train final model with optimal lambda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb789d0f-677b-4ffc-9512-bf6a4120f811",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_model <- glmnet(X, y, alpha = 0, lambda = lambda_opt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96eaee2c-8cd8-43dd-b61d-42f422746359",
   "metadata": {},
   "source": [
    "#### Easier way, use cv.glmnet \n",
    "(even easier, caret -> TA session)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84307874",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Cross-validation using cv.glmnet with the same folds\n",
    "cv_ridge <- cv.glmnet(X, y, alpha = 0, lambda = lambda_seq, foldid = folds)\n",
    "# foldid: an optional vector of values between 1 and nfolds identifying what fold each observation is in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "340c1d9c-0908-4e5d-b03a-a33939929582",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimal lambda from cv.glmnet\n",
    "lambda_opt_glmnet <- cv_ridge$lambda.min\n",
    "print(lambda_opt_glmnet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b769ed",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "cv_ridge"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc31f053",
   "metadata": {},
   "source": [
    "We can plot the MSE for each $\\lambda$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a2d2c39",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "plot(cv_ridge)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76033caf",
   "metadata": {},
   "source": [
    "This plots the cross-validation curve (red dotted line) along with upper and lower standard deviation curves\n",
    "along the $\\lambda$ sequence (error bars). \n",
    "\n",
    "Two special values along the $\\lambda$ sequence are indicated by the vertical dotted lines:\n",
    " - lambda.min is the value of $\\lambda$ that gives minimum mean cross-validated error, while \n",
    " - lambda.1se is the value of $\\lambda$ that gives the most regularized model such that the cross-validated error is within one standard error of the minimum."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83299a64",
   "metadata": {},
   "source": [
    "We can use the following code to get the value of `lambda.min` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6937d823",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "cv_ridge$lambda.min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7094011c",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "log(cv_ridge$lambda.min)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebca54cd",
   "metadata": {},
   "source": [
    "and the model coefficients at that value of $\\lambda$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62b9ae5b",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "head(coef(cv_ridge, s = \"lambda.min\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "878a5775-ccf1-4a9f-a8e0-69a00b045acc",
   "metadata": {},
   "source": [
    "In practice, the 1SE rule is sometimes used because some studies found that models chosen by $\\lambda_{1SE}$ **tend to perform better on test data** than those chosen by $\\lambda_{\\min}$ (Hastie et al., *The Elements of Statistical Learning*)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3f16321",
   "metadata": {},
   "source": [
    "## Lasso\n",
    "\n",
    "Now let's fit lasso, which can be fit using `glmnet()` with alpha = 1 and seeks to minimize:\n",
    "\n",
    "\n",
    "$$\n",
    "\\frac{1}{2n}\\sum_{i=1}^{n} \\left( y_i - \\beta_0 - \\sum_{j=1}^{p} \\beta_j x_{ij}    \\right) ^ 2 + \\lambda \\sum_{j=1}^{p} |\\beta_j|\n",
    "$$\n",
    "\n",
    "Notice that the intercept is not penalized. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1035b84a",
   "metadata": {},
   "source": [
    "### No penalty = OLS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4459e15e",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "lasso_no_pen <- glmnet(\n",
    "  x = Xsmall,\n",
    "  y = y,\n",
    "  alpha = 1, #lasso\n",
    "  lambda=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d6dc1b1",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "lasso_no_pen$beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5d6eef7",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "summary(lm(y~Xsmall))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "366f3037",
   "metadata": {},
   "source": [
    "### With Penalty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88c2d621",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "lasso_pen <- glmnet(\n",
    "  x = Xsmall,\n",
    "  y = y,\n",
    "  alpha = 1, #lasso\n",
    "  lambda=.02\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f8b98d5",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "lasso_pen$beta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0cc8211-bbf9-4101-b53e-1438b42681b3",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "source": [
    "#### Larger Penalty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e2a40c9",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "lasso_pen_large <- glmnet(\n",
    "  x = Xsmall,\n",
    "  y = y,\n",
    "  alpha = 1, #lasso\n",
    "  lambda=1e70\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4146900b",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "lasso_pen_large$beta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6b6817f",
   "metadata": {},
   "source": [
    "#### Various Penalties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "222589f8",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "lasso01 <- glmnet(\n",
    "  x = Xsmall,\n",
    "  y = y,\n",
    "  alpha = 1 #lasso\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25a0dd26",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "plot(lasso01, xvar = \"lambda\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ecd1689-22e5-43e9-a31f-67a9929b3d7a",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "source": [
    "### Penalty selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0d96a62-28c2-4b4f-8b70-f3579cc8b341",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_lasso <- cv.glmnet(X, y, alpha = 1, lambda = lambda_seq, foldid = folds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70d54bfa-f22c-45fb-8b8e-7bdbec5aed86",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(cv_lasso)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95be7c67-1eef-4d1c-8bcd-59783e2dccf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_lasso$lambda.min"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c63bcb5b",
   "metadata": {},
   "source": [
    "## Elastic Net \n",
    "\n",
    "Now let's fit EN, which can be fit using `glmnet()`  and seeks to minimize:\n",
    "\n",
    "$$\n",
    "\\frac{1}{2n}\\sum_{i=1}^{n} \\left( y_i - \\beta_0 - \\sum_{j=1}^{p} \\beta_j x_{ij}    \\right) ^ 2 + \\lambda\\left[(1-\\alpha)\\frac{1}{2}\\|\\beta\\|_2^2 + \\alpha \\|\\beta\\|_1\\right],\n",
    "$$\n",
    "\n",
    "where \n",
    "- If $\\alpha=1$ Lasso \n",
    "- If $\\alpha=0$ Ridge \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a0723ee-f0c1-40e3-b6c1-2a025907883d",
   "metadata": {},
   "source": [
    "- The elastic net simultaneously does automatic variable selection and continuous shrinkage, and it can select groups of correlated variables. \n",
    "\n",
    "- It is like a stretchable fishing net that retains ‘all the big fish’.\n",
    "\n",
    "- Simulation studies and real data examples show that the elastic net often outperforms the lasso in terms of prediction accuracy\n",
    "\n",
    "- The strict convexity part  of the penalty (ridge) solves the grouping instability problem \n",
    "\n",
    "- In **Elastic Net (EN)**, we have **two hyperparameters** to tune:\n",
    "    1. **$\\alpha$**: The mixing parameter between Lasso $(\\alpha = 1)$ and Ridge $(\\alpha = 0)$.\n",
    "    2. **$\\lambda$**: The penalty strength.\n",
    "       \n",
    "    - How to choose $(\\lambda,\\alpha)$? $\\rightarrow$ Bidimensional Crossvalidation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b180d204-ec2d-4a83-b20c-fa5a1f871969",
   "metadata": {},
   "source": [
    "To perform **bidimensional cross-validation** we can follow these steps:\n",
    "\n",
    "1. **Choose a grid of $\\alpha$ values**  from 0 to 1).\n",
    "2. **For each $\\alpha$, perform cross-validation over $\\lambda$** using `cv.glmnet()`.\n",
    "3. **Select the best $\\alpha, \\lambda$ pair** based on cross-validation error.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "299596ab-ea6c-4556-bac1-1320dc5fdc3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define alpha grid (e.g., from 0 to 1 in steps of 0.2) #just to be fast, in practice need a finer grid\n",
    "alpha_grid <- seq(0, 1, by = 0.2)\n",
    "\n",
    "\n",
    "# Initialize storage for results\n",
    "cv_results <- data.frame(alpha = numeric(), lambda = numeric(), mse = numeric())\n",
    "\n",
    "# Perform bidimensional cross-validation\n",
    "for (alpha_value in alpha_grid) {\n",
    "  # Perform cross-validation for each alpha\n",
    "  cv_fit <- cv.glmnet(X, y, alpha = alpha_value, lambda = lambda_seq, foldid = folds) #note we are using the same folds and lambda sequences as before\n",
    "  #how many specifications are we running?\n",
    "    \n",
    "  # Store best lambda and corresponding MSE\n",
    "  best_lambda <- cv_fit$lambda.min\n",
    "  best_mse <- min(cv_fit$cvm)  # Mean CV error\n",
    "  \n",
    "  # Append results\n",
    "  cv_results <- rbind(cv_results, data.frame(alpha = alpha_value, lambda = best_lambda, mse = best_mse))\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d77053e-19d0-4957-9eec-72e63e929cb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fcc2061-4f9f-4d80-b08b-6b5f51f4a2c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the best alpha-lambda combination\n",
    "best_model <- cv_results[which.min(cv_results$mse), ]\n",
    "\n",
    "\n",
    "# Print best alpha and lambda\n",
    "print(paste(\"Best alpha:\", best_model$alpha))\n",
    "print(paste(\"Best lambda:\", best_model$lambda))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eff7df6-4939-442c-9b56-30a8d489e5f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the final model with the best alpha and lambda\n",
    "final_model <- glmnet(X, y, alpha = best_model$alpha, lambda = best_model$lambda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8333f2d7-78ab-4c58-8b1c-f41828acfe1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# coef(final_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b88c013c-42ff-4937-a295-8a8acf5f6fdc",
   "metadata": {},
   "source": [
    "- **Elastic Net performs best when $\\alpha$ is optimized** rather than arbitrarily chosen.\n",
    "- **Allows flexibility between Lasso and Ridge**, automatically adjusting to the dataset.\n",
    "- **Prevents overfitting or underfitting** by tuning both hyperparameters simultaneously.\n"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,Rmd"
  },
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.4.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
