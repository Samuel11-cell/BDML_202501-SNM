{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a952ad54-2218-4382-b2eb-d401d349f764",
   "metadata": {},
   "source": [
    "<div >\n",
    "<img src = \"../banner.jpg\" />\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65198d8b",
   "metadata": {},
   "source": [
    "<a target=\"_blank\" href=\"https://colab.research.google.com/github/ignaciomsarmiento/BDML_202501/blob/main/Modulo05/CuadernoModulo05_ArbolesBosques.ipynb\">\n",
    "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
    "</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7e57de4",
   "metadata": {},
   "source": [
    "# Tree-Based Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "303a3dcd",
   "metadata": {},
   "source": [
    "## Motivación: Prediciendo Precios de propiedades\n",
    "\n",
    "\n",
    "$$\n",
    "Precio=f(Habitaciones,DCBD)\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9183a2e",
   "metadata": {},
   "source": [
    "## CARTs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fdab5a6",
   "metadata": {},
   "source": [
    "Cargamos los paquetes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dce4bb18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# install.packages(\"pacman\") #run this line if you use Google Colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e94ec2b-ab94-4ec8-8512-e370d2f22a56",
   "metadata": {
    "tags": [
     "remove_cell"
    ]
   },
   "outputs": [],
   "source": [
    "#packages\n",
    "require(\"pacman\")\n",
    "p_load(\"tidyverse\",\"ggplot2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d6b3e2f",
   "metadata": {},
   "source": [
    "### Toy example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41492fd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "db<-read.csv('https://raw.githubusercontent.com/ignaciomsarmiento/datasets/main/toy_houses.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21942630",
   "metadata": {},
   "outputs": [],
   "source": [
    "dim(db)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efd33e96",
   "metadata": {},
   "outputs": [],
   "source": [
    "head(db)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ea12722",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary(db)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdf4ce56",
   "metadata": {},
   "outputs": [],
   "source": [
    "ggplot(db) +\n",
    "  geom_point(aes(x=habitaciones,y=DCBD),position=position_jitter(width = .05)) +\n",
    "  scale_x_continuous(breaks=seq(0,8,1)) +\n",
    "  theme_classic() +\n",
    "  xlab(\"Habitaciones\") +\n",
    "  ylab(\"Distancia al Centro\") +\n",
    "  theme(legend.position =  \"none\",\n",
    "      text=element_text(size=20))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09553e79",
   "metadata": {},
   "source": [
    "#### Algorithm\n",
    "\n",
    "\n",
    "-  Datos: $y_{n\\times 1}$  y $X_{n\\times k}$ \n",
    "\n",
    "-  Definiciones\n",
    "\n",
    "      -  *j* es la variable que parte el espacio \n",
    "      - *s* es el punto de partición\n",
    "\n",
    "\n",
    "-  Definimos los siguientes semiplanos\n",
    "\n",
    "\\begin{align}\n",
    "R_1(j,s)=\\{X|X_j\\leq s\\} \\,\\,\\, \\& \\,\\,\\, R_2(j,s)=\\{X|X_j > s\\}\n",
    "\\end{align}\n",
    "\n",
    "-  *El problema*: buscar la variable de partición $X_j$ y el punto $s$ de forma tal que \n",
    "\n",
    "\n",
    "\\begin{align}\n",
    "\\underset{j,s}{min} \\left[ \\underset{y_{R_1}}{min}\\sum_{x_i\\in R_1(j,s)}(y-y_{R_1})^2+ \\underset{y_{R_2}}{min}\\sum_{x_i\\in R_2(j,s)}(y-y_{R_2})^2\\right]\n",
    "\\end{align}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8277129d",
   "metadata": {},
   "source": [
    "#### Algorithm by hand (\"artesanal\")\n",
    "\n",
    "1. Iniciemos por DBCD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5309b10e",
   "metadata": {},
   "outputs": [],
   "source": [
    "MSE_dbcd<-NA\n",
    "\n",
    "j<-1\n",
    "for(i in seq(1.25,1.75,0.25)){\n",
    "    #Region 1\n",
    "  R1<- db %>% filter(DCBD<=i)\n",
    "  R1<- R1 %>% mutate(yR1=mean(price))\n",
    "  MSEr1<- sum((R1$price-R1$yR1)^2)\n",
    " #Region 2\n",
    "  R2<- db %>% filter(DCBD>i)\n",
    "  R2<- R2 %>% mutate(yR2=mean(price))\n",
    "  MSEr2<- sum((R2$price-R2$yR2)^2)\n",
    "  \n",
    "  MSE_dbcd[j]<-MSEr1+MSEr2\n",
    "  j<-j+1\n",
    "}\n",
    "\n",
    "MSE_dbcd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f109b93d",
   "metadata": {},
   "source": [
    "2. Luego por Habitaciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "350493dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "MSE_hab<-NA\n",
    "\n",
    "for(i in 0:7){\n",
    "  R1<- db %>% filter(habitaciones<=i)\n",
    "  R1<- R1 %>% mutate(yR1=mean(price))\n",
    "  MSEr1<- sum((R1$price-R1$yR1)^2)\n",
    "  R2<- db %>% filter(habitaciones>i)\n",
    "  R2<- R2 %>% mutate(yR2=mean(price))\n",
    "  MSEr2<- sum((R2$price-R2$yR2)^2)\n",
    "  \n",
    "  MSE_hab[i+1]<-MSEr1+MSEr2\n",
    "  \n",
    "}\n",
    "MSE_hab"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbddcfc4",
   "metadata": {},
   "source": [
    "**Mínimo?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1637a9b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "MSE<-c(MSE_dbcd,MSE_hab)\n",
    "MSE[which.min(MSE)]\n",
    "MSE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19344215",
   "metadata": {},
   "source": [
    "#### Algorithm with `rpart`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ed59183-97a3-4149-972e-a1544b7ddf47",
   "metadata": {},
   "source": [
    "Sin embargo, no construimos el árbol a mano, usamos el paquete `rpart`, que implementa el algoritmo de árboles de decisión para regresión y clasificación. Este paquete permite ajustar modelos de manera rápida y sencilla:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a7c95aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "p_load(\"rpart\")\n",
    "mytree <- rpart(price~DCBD+habitaciones,data=db)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d274c14e",
   "metadata": {},
   "outputs": [],
   "source": [
    "mytree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30b93124",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(mytree)\n",
    "text(mytree)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e4232d6-2560-4d64-a257-bc5ffdc76b33",
   "metadata": {},
   "source": [
    "Para visualizar árboles de decisión de forma mas \"bonita\", podemos usar el paquete `rpart.plot`, que extiende las capacidades de rpart con funciones gráficas más potentes y estéticas. La función principal es `prp()` qye permite crear gráficos detallados del árbol entrenado. En el siguiente ejemplo, usamos varias opciones para mejorar la presentación del árbol:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54712843",
   "metadata": {},
   "outputs": [],
   "source": [
    "p_load(\"rpart.plot\")\n",
    "\n",
    "prp(mytree, \n",
    "    under = TRUE,              # Muestra información adicional debajo del nodo\n",
    "    branch.lty = 2,            # Estilo de línea punteada para las ramas\n",
    "    yesno = 2,                 # Muestra \"sí/no\" en las bifurcaciones\n",
    "    faclen = 0,                # Muestra etiquetas completas de factores\n",
    "    varlen = 15,               # Longitud máxima del nombre de la variable\n",
    "    tweak = 1.2,               # Ajusta el tamaño del texto\n",
    "    clip.facs = TRUE,          # Recorta niveles largos de factores\n",
    "    box.palette = \"Greens\",    # Paleta de colores para las cajas\n",
    "    compress = TRUE,           # Comprime el árbol verticalmente\n",
    "    ycompress = TRUE,          # Comprime también el eje y\n",
    "    node.fun = function(x, labs, digits, varlen) \n",
    "      paste(\"Precio \\n\", format(round(mytree$frame$yval, 2), nsmall=0, big.mark=\",\"))  # Personaliza el texto del nodo\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c779cd7-3ffe-41af-b2d4-99d9a8bc844e",
   "metadata": {},
   "source": [
    "Recordemos que la forma general del arbol es\n",
    "$$\n",
    "f(x_{i};\\{R_{j},\\gamma_{j}\\}_{1}^{J})=\\sum_{j=1}^{J}\\gamma_{j}I(x\\in R_{j})\n",
    "$$\n",
    "\n",
    "Entonces en este caso, el precio predicho por el  árbol es \n",
    "\n",
    "\\begin{align}\n",
    "\\hat{Precio} = 162,754.67 I(DCBD<1.5) + 98,716.14 I(DCBD>=1.5 \\& habitaciones>=3) + 73,130.58 I(DCBD>=1.5 \\& habitaciones<3) \n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f08f3fa",
   "metadata": {},
   "source": [
    "### Ames Data Set\n",
    "\n",
    "Este conjunto de datos contiene información detallada sobre las características de viviendas en Ames, Iowa, y su precio de venta. Incluye más de 80 variables que describen aspectos físicos, de ubicación y de calidad de las propiedades, como el tamaño habitable, el tipo de edificio, el año de construcción, el número de baños, y si tiene garaje o cercado, entre otros.\n",
    "\n",
    "Es ampliamente utilizado en ciencia de datos y machine learning como un caso realista para problemas de regresión.  \n",
    "La descripción completa de las variables puede consultarse aquí:  \n",
    "https://jse.amstat.org/v19n3/decock/DataDocumentation.txt\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "647d6dcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "p_load(\"modeldata\")\n",
    "\n",
    "data(\"ames\", package = \"modeldata\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68a3997a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dim(ames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5b90bab",
   "metadata": {},
   "outputs": [],
   "source": [
    "head(ames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca09a27a",
   "metadata": {},
   "outputs": [],
   "source": [
    "amestree <- rpart(log(Sale_Price) ~  Gr_Liv_Area  + Bldg_Type + Fence + Lot_Area,data=ames, control = list(maxdepth = 2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e1c1469-4bb1-4561-9d99-f0c09d9034ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "p_load(\"rpart.plot\")\n",
    "prp(amestree, under = TRUE, branch.lty = 2, yesno = 2, faclen = 0, varlen=15,tweak=1.2,clip.facs= TRUE,box.palette = \"Greens\",compress=FALSE,ycompress = FALSE,node.fun=function(x, labs, digits, varlen) paste(\"Precio \\n\", format(round(exp(amestree$frame$yval), 0), nsmall=0, big.mark=\",\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de10c96b",
   "metadata": {},
   "source": [
    "### Sobreajuste"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bac01b4",
   "metadata": {},
   "source": [
    "<div >\n",
    "<img src = \"figures/tree_uba.png\" width=\"300\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dddbde6",
   "metadata": {},
   "source": [
    "\n",
    "- Fijar la profundidad del árbol. (implementado en Caret `method=rpart2`)\n",
    "\n",
    "- Fijar la mínima cantidad de datos que están contenidos dentro de cada hoja. \n",
    "\n",
    "- Cost complexity pruning (implementado en Caret con `method=rpart`)\n",
    "\n",
    "- `tidymodels` implementa todos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e92e07f",
   "metadata": {},
   "source": [
    "##### Implementación con Caret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecfc1c57",
   "metadata": {},
   "outputs": [],
   "source": [
    "p_load(\"caret\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "318c6c70",
   "metadata": {},
   "outputs": [],
   "source": [
    "fitControl<-trainControl(method =\"cv\",\n",
    "                         number=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3c8e9ce",
   "metadata": {},
   "source": [
    "#####  `method=rpart2`  allows to tune Max Tree Depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5df693c",
   "metadata": {},
   "outputs": [],
   "source": [
    "set.seed(123)\n",
    "tree_rpart2 <- train(\n",
    "    log(Sale_Price) ~ Gr_Liv_Area  + Bldg_Type + Fence + Lot_Area,\n",
    "    data=ames,\n",
    "    method = \"rpart2\",\n",
    "    trControl = fitControl,\n",
    "    tuneGrid = expand.grid(maxdepth = seq(1,8,1))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffaf5931",
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_rpart2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9627361",
   "metadata": {},
   "outputs": [],
   "source": [
    "prp(tree_rpart2$finalModel, under = TRUE, branch.lty = 2, yesno = 2, faclen = 0, varlen=15,tweak=1.2,clip.facs= TRUE,box.palette = \"Greens\",compress=FALSE,ycompress = FALSE,node.fun=function(x, labs, digits, varlen) paste(\"Precio \\n\", format(round(exp(tree_rpart2$finalModel$frame$yval), 0), nsmall=0, big.mark=\",\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "135403a9",
   "metadata": {},
   "source": [
    "#### Cost complexity Prunning\n",
    "\n",
    "\n",
    "Cost complexity del árbol  $T$ con $[T]$ nodos terminales del árbol \n",
    "\\begin{align}\n",
    "  C_{\\alpha}(T)= \\sum_{m=1}^{[T]}  \\sum_{x_i\\in R_m} (y_i-\\hat{y}_m)^2 + \\alpha [T]\n",
    "\\end{align}\n",
    "\n",
    "\n",
    "Objetivo: para un dado $\\alpha$, encontrar el pruning óptimo que minimice  $C_{\\alpha}(T)$\n",
    "\n",
    "Se logra eliminando sucesivamente las ramas que producen un aumento mínimo en $\\sum_{x_i\\in R_m} (y_i-\\hat{y}_m)^2 $\n",
    "\n",
    "\n",
    "##### Algoritmo completo\n",
    "\n",
    "  - Hacemos crecer el árbol\n",
    "\n",
    "  - Para un dado $\\alpha$, aplicamos  *cost complexity pruning* \n",
    "    \n",
    "  - Utilizamos K-fold cross-validation para elegir $\\alpha$. \n",
    "\n",
    "  \n",
    "Tenemos entonces una secuencia de subarboles para distintos valores de $\\alpha$ \n",
    "\n",
    "Elegimos el $\\alpha$ y el subárbol que tienen el menor error de predicción."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfd6d093",
   "metadata": {},
   "source": [
    "#####  `method=rpart`  only allows to tune Complexity Parameter\n",
    "\n",
    "- Can change the length\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c93df854",
   "metadata": {},
   "outputs": [],
   "source": [
    "set.seed(123)\n",
    "tree_lenght <- train(\n",
    "    log(Sale_Price) ~ Gr_Liv_Area  + Bldg_Type + Fence + Lot_Area,\n",
    "    data=ames,\n",
    "    method = \"rpart\",\n",
    "    trControl = fitControl,\n",
    "    tuneLength=20\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1c61d27",
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_lenght"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "962e34fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "prp(tree_lenght$finalModel, under = TRUE, branch.lty = 2, yesno = 2, faclen = 0, varlen=15,tweak=1.2,clip.facs= TRUE,box.palette = \"Greens\",compress=FALSE,ycompress = FALSE,node.fun=function(x, labs, digits, varlen) paste(\"Precio \\n\", format(round(exp(tree_lenght$finalModel$frame$yval), 0), nsmall=0, big.mark=\",\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82c16df7",
   "metadata": {},
   "source": [
    "- Or the grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5bda02f",
   "metadata": {},
   "outputs": [],
   "source": [
    "set.seed(123)\n",
    "tree_grid <- train(\n",
    "    log(Sale_Price) ~ Gr_Liv_Area  + Bldg_Type + Fence + Lot_Area,\n",
    "    data=ames,\n",
    "    method = \"rpart\",\n",
    "    trControl = fitControl,\n",
    "    tuneGrid = expand.grid(cp = seq(0.001707763, 0.001707765, length.out = 100))\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9169c095",
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_grid"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e563acfa",
   "metadata": {},
   "source": [
    "More details here: https://topepo.github.io/caret/train-models-by-tag.html#tree-based-model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "906ba4df",
   "metadata": {},
   "source": [
    "### Comentarios sobre Árboles\n",
    "\n",
    "\n",
    "#### Pros: \n",
    "  \n",
    "    - Los árboles son muy fáciles de explicar a las personas (probablemente incluso más fáciles que la regresión lineal)\n",
    "\n",
    "    - Los árboles se pueden trazar gráficamente y son fácilmente interpretados incluso por no expertos. Variables más importantes en la parte superior\n",
    "\n",
    "\n",
    "\n",
    "#### Cons:\n",
    "    \n",
    "    - Si la estructura es lineal, CART no funciona bien\n",
    "    \n",
    "<div >\n",
    "<img src = \"figures/tree_vs_reg.png\" />\n",
    "</div>\n",
    "\n",
    "\n",
    "    - Los árboles no son muy robustos \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2e1a513",
   "metadata": {},
   "source": [
    "## Bagging and Random Forests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9892820e-2695-4b38-bab7-c5743fd314c3",
   "metadata": {},
   "source": [
    "### Intuición con código\n",
    "\n",
    "\n",
    "Una forma de mejorar la estabilidad y precisión de los árboles de decisión es a través del bagging (bootstrap aggregating). La idea central es entrenar múltiples árboles sobre distintas muestras del conjunto de datos —obtenidas con reemplazo (bootstrap)— y luego combinar sus predicciones. Esto reduce la varianza del modelo y mejora su capacidad de generalización.\n",
    "\n",
    "En este ejemplo, generamos una única muestra bootstrap del dataset original y entrenamos un árbol simple sobre esa muestra:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcaeb7fd-ad15-4b3d-b296-d385f806024d",
   "metadata": {},
   "outputs": [],
   "source": [
    "set.seed(101010)\n",
    "sample_ames2 <- sample_frac(ames, size = 1, replace = TRUE) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b55edcca-353a-4c16-bbd9-05d6ffe772e9",
   "metadata": {},
   "source": [
    "Entrenamos un árbol poco profundo (maxdepth = 2) para facilitar la visualización y destacar cómo el modelo aprende patrones sobre una muestra específica del conjunto. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6643443e-f697-4930-a5dd-17c0dbdb4ced",
   "metadata": {},
   "outputs": [],
   "source": [
    "amestree2 <- rpart(log(Sale_Price) ~ Gr_Liv_Area + Bldg_Type + Fence + Lot_Area,\n",
    "                   data = sample_ames2,\n",
    "                   control = list(maxdepth = 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63455d30-55ed-411d-8b40-d6d45284fa3a",
   "metadata": {},
   "source": [
    "Luego, visualizamos el árbol con `prp()` personalizando las etiquetas de los nodos para mostrar el precio estimado en escala original (deshaciendo el log):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70e3e44f-d3a4-4087-9d41-e8e30debc128",
   "metadata": {},
   "outputs": [],
   "source": [
    "prp(amestree2, \n",
    "    under = TRUE, branch.lty = 2, yesno = 2, faclen = 0, varlen = 15, tweak = 1.2,\n",
    "    clip.facs = TRUE, box.palette = \"Greens\", compress = FALSE, ycompress = FALSE,\n",
    "    node.fun = function(x, labs, digits, varlen) \n",
    "      paste(\"Precio \\n\", format(round(exp(amestree2$frame$yval), 0), nsmall = 0, big.mark = \",\"))\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d7c2ecb-f745-4aa6-9f70-2b2653c5e86c",
   "metadata": {},
   "source": [
    "Comparamos el árbol ajustado sobre la muestra bootstrap (`amestree2`) con el árbol original (`amestree`), entrenado sobre el conjunto de datos original. Esta comparación nos permite visualizar cómo el muestreo con reemplazo introduce variabilidad en la estructura del árbol —una idea clave detrás del bagging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec4ceeff-1479-4835-bb29-25a8d8b48c30",
   "metadata": {},
   "outputs": [],
   "source": [
    "prp(amestree, under = TRUE, branch.lty = 2, yesno = 2, faclen = 0, varlen=15,tweak=1.2,clip.facs= TRUE,box.palette = \"Greens\",compress=FALSE,ycompress = FALSE,node.fun=function(x, labs, digits, varlen) paste(\"Precio \\n\", format(round(exp(amestree$frame$yval), 0), nsmall=0, big.mark=\",\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c2239c2-df00-41be-b57d-1d6f04909aa1",
   "metadata": {},
   "source": [
    "Como vemos formalmente las predicciones de Bagging?\n",
    "\n",
    "Formalmente, si denotamos los árboles como funciones:\n",
    "\n",
    "$$\n",
    "T_{\\text{1}}(x) = \\sum_{j=1}^{J_1} \\gamma_{j}^{(1)} \\cdot \\mathbf{1}(x \\in R_j^{(1)}), \\quad\n",
    "T_{\\text{2}}(x) = \\sum_{k=1}^{J_2} \\gamma_{k}^{(2)} \\cdot \\mathbf{1}(x \\in R_k^{(2)})\n",
    "$$\n",
    "\n",
    "donde cada $ R_j^{(1)} $ y $ R_k^{(2)} $ es una región terminal (una hoja) del árbol correspondiente, y $ \\gamma 4 es la predicción constante en esa región.\n",
    "\n",
    "Entonces, si quisiéramos combinar estos dos árboles, la predicción combinada sería simplemente el promedio:\n",
    "\n",
    "$\n",
    "\\hat{f}(x) = \\frac{1}{2} \\left( T_{\\text{1}}(x) + T_{\\text{2}}(x) \\right)\n",
    "$\n",
    "\n",
    "En términos prácticos, para cualquier observación \\( x \\), cada árbol le asigna un valor predicho dependiendo de la región hoja en la que cae, y el modelo final sería el promedio de ambos.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23baef3a-65ab-40ff-ad7e-6a8ec4bfa828",
   "metadata": {},
   "source": [
    "### Con `caret` y `ranger`\n",
    "\n",
    "Podemos entrenar estos modelos  utilizando el paquete `ranger` en combinación con `caret`, lo cual nos permite buscar automáticamente la mejor combinación de hiperparámetros mediante validación cruzada.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34e99d21-bc30-4a50-8a7d-c272a30ea8f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "p_load(\"ranger\")\n",
    "p_load(\"caret\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4727e165-e5d6-4cc8-9c9a-1f6c82f3f024",
   "metadata": {},
   "source": [
    "\n",
    "`Caret` nos permite ajustar facilmente los siguientes hiperparámetros\n",
    "\n",
    "- `mtry`: número de predictores seleccionados aleatoriamente en cada división.\n",
    "- `splitrule`: criterio de división (en regresión, típicamente `\"variance\"`).\n",
    "- `min.node.size`: número mínimo de observaciones en un nodo terminal.\n",
    "- Falta alguno importante?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e876df0",
   "metadata": {},
   "outputs": [],
   "source": [
    "set.seed(123)  # Fijamos semilla para reproducibilidad\n",
    "\n",
    "tree_ranger_grid <- train(\n",
    "    log(Sale_Price) ~ Gr_Liv_Area + Bldg_Type + Fence,  # Fórmula del modelo\n",
    "    data = ames,  # Dataset de entrenamiento\n",
    "    method = \"ranger\",  # Usamos el motor ranger para Random Forests\n",
    "    trControl = fitControl,  # Especificamos los controles de validación cruzada definidos antes\n",
    "    tuneGrid = expand.grid(   # Definimos la grilla de hiperparámetros a explorar\n",
    "        mtry = c(1, 2, 3),  # Número de predictores seleccionados al azar en cada división\n",
    "        splitrule = \"variance\",  # Regla de partición basada en la reducción de varianza (regresión)\n",
    "        min.node.size = c(1, 3, 5)  # Tamaño mínimo de nodos terminales\n",
    "    )#,\n",
    "    #importance = \"permutation\"  # Calculamos la importancia de variables por permutación\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30258a75",
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_ranger_grid"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ccadb0f",
   "metadata": {},
   "source": [
    "Una vez entrenado el modelo, podemos examinar qué tan importantes fueron las variables explicativas en las decisiones del Random Forest. Esto nos da una idea de qué características influyen más en la predicción del precio de las viviendas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f55fa8bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#varImp(tree_ranger_grid)"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,Rmd"
  },
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.4.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
